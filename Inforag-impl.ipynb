{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1145f145-ed0c-403d-8751-924b2f55fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bf321-aceb-4baa-9a51-ca4bc1381413",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fd863e-e9aa-482f-abee-c94d56717c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = joblib.load(\"../INFO-RAG/uns_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f334f48-eb9e-4469-9b1a-4ad6b3cc3f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2046737"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1efa6-86a4-4244-a94d-9f0b2c79488d",
   "metadata": {},
   "source": [
    "### only get 100 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e9dbee-d198-4719-8c7c-a860561058ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_list[:100]\n",
    "len(data)\n",
    "#len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a473d8-9744-4ffb-a745-745d7a643ef4",
   "metadata": {},
   "source": [
    "### InfoRAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a27546d-9647-4f0d-b415-ea96fd08fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "def tokenize(\n",
    "        prompt,\n",
    "        completion,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "):\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    source_output = tokenizer.encode(prompt)\n",
    "    input_seq = prompt + ' ' + completion\n",
    "    passage_list = prompt\n",
    "    tokenize_output = tokenizer(input_seq, padding=False, return_tensors=None,max_length=512,truncation=False)\n",
    "    passage_list_tokenize_output = tokenizer(passage_list, padding=False, return_tensors=None, max_length=512, truncation=False)\n",
    "    IGNORE_INDEX = -100\n",
    "    source_len = len(source_output) - 1\n",
    "\n",
    "    tokenize_output[\"labels\"] = copy.deepcopy(tokenize_output[\"input_ids\"])\n",
    "    tokenize_output[\"labels\"] = [IGNORE_INDEX] * source_len + tokenize_output[\"labels\"][source_len:]\n",
    "    return passage_list_tokenize_output,tokenize_output\n",
    "\n",
    "special_token_list = [1,32000,32001]\n",
    "import random\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_type,\n",
    "                 data_list):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_list = data_list\n",
    "\n",
    "        if data_type == 'train':\n",
    "            self.data_list = self.data_list[:int(1.0*len(self.data_list))]\n",
    "        else:\n",
    "            self.data_list = self.data_list[int(0.2*len(self.data_list))+1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i % (1000) == 0 and int(-1) == 0:\n",
    "            sp = subprocess.Popen([\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            out_str = sp.communicate()\n",
    "            for out_element in out_str:\n",
    "                for line in str(out_element).split('\\\\n'):\n",
    "                    print(line, file=sys.stderr)\n",
    "        return self.data_list[i]\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c05ae1dd-6943-4ed2-9c37-130938eb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model_name_or_path = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#   model_name_or_path, fast_tokenizer=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# # make sure tokenizer is right pad in our logic\n",
    "# tokenizer.padding_side = 'right'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffec85-abfe-4128-95b9-0e24914cb4a7",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7619e2a-55a0-44e9-b05d-11fdb78b2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0f933c-a77e-456c-85de-238eeac6e27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "0\n",
      "128001\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.padding_side)\n",
    "print(tokenizer.pad_token_type_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1b0208-3811-46a2-ba66-788705db4816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['[MASK_PASSAGE]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c655f3-792a-4bdd-9e7b-38e06664a08a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b154c5a9-cbe1-4bb0-89f2-633a07a2d388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import math\n",
    "\n",
    "train_dataset = SupervisedDataset(tokenizer=tokenizer,data_type='train',data_list=data)\n",
    "train_sampler = SequentialSampler(train_dataset)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  collate_fn=data_collator,\n",
    "                                  sampler=train_sampler,\n",
    "                                  shuffle=False,\n",
    "                                  batch_size=4)\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / 0.1) # the higher the lower steps/epoch\n",
    "\n",
    "num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01109f24-425a-4259-9789-5671bfbd2490",
   "metadata": {},
   "source": [
    "### Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e3732c-df63-41c9-b3e1-a23c1045b101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aaron the priesthood for himself and his male descendants, and he became the first High Priest of the Israelites.This content is generated according to my knowledge and [REFERENCE] number 8\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([26757, 279, 86716, 369, 5678, 323, 813, 8762, 49446, 11, 323, 568, 6244, 279, 1176, 5234, 50003, 315, 279, 6921, 3695, 13, 2028, 2262, 374, 8066, 4184, 311, 856, 6677, 323, 510, 82176, 60, 1396, 220, 23]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e37d57ff-40b1-4e58-bcb8-1cbc1bdf67a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 85.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "instances_new = []\n",
    "data_tag = 0\n",
    "\n",
    "datasets = []\n",
    "for step, data_list in tqdm(enumerate(train_dataloader)):\n",
    "    ### data start ###\n",
    "    instances_new = []\n",
    "    data_tag = 0\n",
    "    for i in range(len(data_list)):\n",
    "        if step%10 < 4:  # Correct and Complete\n",
    "            data_tag = 0\n",
    "            special_token = '[REFERENCE]'\n",
    "            passage_list = data_list[i][2]\n",
    "            score_list = data_list[i][3]\n",
    "            passage_ids_list = []\n",
    "            for passage in passage_list:\n",
    "                input_passage_text = (special_token + ' ' + passage)\n",
    "                output_passage, output_all_token = tokenize(input_passage_text, '',\n",
    "                                                            tokenizer)\n",
    "                output_passage_token_ids = output_passage['input_ids']\n",
    "                label_ids = output_passage_token_ids\n",
    "\n",
    "                mask_probability = 0.3 #process 30% tokens\n",
    "\n",
    "                masked_indices = [i for i in range(int(len(output_passage_token_ids) / 3),len(output_passage_token_ids)) if\n",
    "                                  random.random() < mask_probability]\n",
    "                masked_ids = []\n",
    "                idx = 0\n",
    "                masked_idx = []\n",
    "                while idx < len(output_passage_token_ids):\n",
    "                    if (not (idx in masked_indices)) or (output_passage_token_ids[idx] in special_token_list):\n",
    "                        masked_ids.append(output_passage_token_ids[idx])\n",
    "                        idx += 1\n",
    "                    else:  # Process two tokens consecutively\n",
    "                        rand_num = random.random()\n",
    "                        if rand_num < 0.5:  # [MASK]\n",
    "                            masked_ids.append(32000)\n",
    "                        elif rand_num > 0.5 and rand_num < 0.6:  # Keep\n",
    "                            masked_ids.append(output_passage_token_ids[idx])\n",
    "                        else:\n",
    "                            masked_ids.append(random.randint(3, 31999))  # Replace\n",
    "                        masked_idx.append(idx)\n",
    "                        idx += 1\n",
    "                        if idx < len(output_passage_token_ids) and (\n",
    "                        not (output_passage_token_ids[idx] in special_token_list)):\n",
    "                            rand_num = random.random()\n",
    "                            if rand_num < 0.5:  # [MASK]\n",
    "                                masked_ids.append(32000)\n",
    "                            elif rand_num > 0.5 and rand_num < 0.6:  # Keep\n",
    "                                masked_ids.append(output_passage_token_ids[idx])\n",
    "                            else:\n",
    "                                masked_ids.append(random.randint(3, 31999))  # Replace\n",
    "                            masked_idx.append(idx)\n",
    "                            idx += 1\n",
    "                passage_ids_list.append((label_ids, masked_ids))\n",
    "\n",
    "            query_ids = tokenize(\n",
    "                'Complete this text according to the above [REFERENCE]: ' + data_list[i][1], '',\n",
    "                tokenizer)[0]['input_ids'][1:] # remove start token\n",
    "            selected_s_i = 0\n",
    "            for s_i in range(len(score_list)):\n",
    "                if score_list[s_i] == 0:\n",
    "                    selected_s_i = s_i\n",
    "                    break\n",
    "            trace_ids = tokenize(\n",
    "                'This content is generated according to my knowledge and [REFERENCE] number {}'.format(selected_s_i), '',\n",
    "                tokenizer)[0]['input_ids'][1:] # remove start token\n",
    "            start_generation = random.randint(int(len(query_ids) / 2),int((3 * len(query_ids)) / 4))\n",
    "            answer_label_ids = [IGNORE_INDEX]*start_generation + query_ids[start_generation:] + trace_ids\n",
    "            input_passage_ids = [1]\n",
    "            origin_ids = [1]\n",
    "            for item in passage_ids_list:\n",
    "                input_passage_ids += item[1][1:]\n",
    "                origin_ids += item[0][1:]\n",
    "            input_ids = input_passage_ids + query_ids + trace_ids\n",
    "            labels = [IGNORE_INDEX] * len(input_passage_ids) + answer_label_ids\n",
    "\n",
    "            #print(\"CoC\",tokenizer.decode(input_ids),\"\\n\")\n",
    "            #print(\"CoC ans\",tokenizer.decode(query_ids[start_generation:] + trace_ids),\"\\n\")\n",
    "            datasets.append(input_ids)\n",
    "\n",
    "        elif step%10 >= 4 and step%10 < 8:  # Contextual Stimulation\n",
    "            data_tag = 1\n",
    "            special_token = '[REFERENCE]'\n",
    "            passage_list = data_list[i][2]\n",
    "            selected_passage = '[QUERY] ' + data_list[i][1]\n",
    "            score_list = data_list[i][3]\n",
    "            score_list_fuben = []\n",
    "            for s in score_list:\n",
    "                if not s == 0:\n",
    "                    score_list_fuben.append(s)\n",
    "            if len(score_list) > 1:\n",
    "                score_list = score_list_fuben\n",
    "            passage_ids_list = []\n",
    "            for passage in passage_list:\n",
    "                input_passage_text = (special_token + ' ' + passage)\n",
    "                output_passage, output_all_token = tokenize(input_passage_text, selected_passage,\n",
    "                                                            tokenizer)\n",
    "                output_passage_token_ids = output_passage['input_ids']\n",
    "                label_ids = output_passage_token_ids\n",
    "                if passage == data_list[i][1]:\n",
    "                    selected_ids = label_ids\n",
    "                else:\n",
    "                    passage_ids_list.append(label_ids)\n",
    "\n",
    "            query_ids = tokenize(\n",
    "                'Complete this text according to the above [REFERENCE]: ' + data_list[i][1], '',\n",
    "                tokenizer)[0]['input_ids'][1:] # remove start token\n",
    "            selected_s_i = 0\n",
    "            for s_i in range(len(score_list)):\n",
    "                if score_list[s_i] == 0:\n",
    "                    selected_s_i = s_i\n",
    "                    break\n",
    "            trace_ids = tokenize(\n",
    "                'This content is generated according to my knowledge'.format(selected_s_i), '',\n",
    "                tokenizer)[0]['input_ids'][1:] # remove start token\n",
    "            start_generation = random.randint(int(len(query_ids) / 2),int((3 * len(query_ids)) / 4))\n",
    "            answer_label_ids = [IGNORE_INDEX]*start_generation + query_ids[start_generation:] + trace_ids\n",
    "            input_passage_ids = [1]\n",
    "            origin_ids = [1]\n",
    "            for item in passage_ids_list:\n",
    "                input_passage_ids += item[1:]\n",
    "                origin_ids += item[1:]\n",
    "            input_ids = input_passage_ids + query_ids + trace_ids\n",
    "            labels = [IGNORE_INDEX] * len(input_passage_ids) + answer_label_ids\n",
    "\n",
    "            #print(\"CS\",tokenizer.decode(input_ids),\"\\n\")\n",
    "            #print(\"CS ans\", tokenizer.decode(query_ids[start_generation:] + trace_ids),\"\\n\")\n",
    "            datasets.append(input_ids)\n",
    "\n",
    "\n",
    "\n",
    "        else:  # Select and Copy\n",
    "            data_tag = 2\n",
    "            special_token = '[REFERENCE]'\n",
    "            passage_list = data_list[i][2]\n",
    "            selected_passage = '[QUERY] ' + data_list[i][1]\n",
    "            score_list = data_list[i][3]\n",
    "            passage_ids_list = []\n",
    "            for passage in passage_list:\n",
    "                input_passage_text = (special_token + ' ' + passage)\n",
    "                output_passage, output_all_token = tokenize(input_passage_text, selected_passage,\n",
    "                                                            tokenizer)\n",
    "                output_passage_token_ids = output_passage['input_ids']\n",
    "                label_ids = output_passage_token_ids\n",
    "                passage_ids_list.append(label_ids)\n",
    "            query_ids = tokenize(\n",
    "                'Complete this text according to the above [REFERENCE]: ' + data_list[i][1], '',\n",
    "                tokenizer)[0]['input_ids'][1:] # remove start token\n",
    "            selected_s_i = 0\n",
    "            for s_i in range(len(score_list)):\n",
    "                if score_list[s_i] == 0:\n",
    "                    selected_s_i = s_i\n",
    "                    break\n",
    "            trace_ids = tokenize(\n",
    "                'This content is generated according to [REFERENCE] number {}'.format(selected_s_i), '',\n",
    "                tokenizer)[0]['input_ids'][1:] # remove start token\n",
    "            start_generation = random.randint(int(len(query_ids) / 2),int((3 * len(query_ids)) / 4))\n",
    "            answer_label_ids = [IGNORE_INDEX]*start_generation + query_ids[start_generation:] + trace_ids\n",
    "            input_passage_ids = [1]\n",
    "            for item in passage_ids_list:\n",
    "                input_passage_ids += item[1:]\n",
    "            input_ids = input_passage_ids + query_ids + trace_ids\n",
    "            labels = [IGNORE_INDEX] * len(input_passage_ids) + answer_label_ids\n",
    "\n",
    "            #print(\"SC\",tokenizer.decode(input_ids),\"\\n\")\n",
    "            #print(\"SC ans\", tokenizer.decode(query_ids[start_generation:] + trace_ids),\"\\n\")\n",
    "            datasets.append(input_ids)\n",
    "        #print(tokenizer.decode(input_ids))\n",
    "        #end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce6db0-ea1e-4883-b80a-b58c3e2644c6",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d911d-e52b-42d9-9e57-6edfc52b63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abbcf567-ed9a-420b-b3af-c84ab2a6d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[REFERENCE] number 0 Aaron Aaron ( or ; \"Ahutility mom\")utility Queensutility, high Southernutility and the_Wutility Moses in the Abrahutilityutility.[REFERENCE] number 1 Knowledge of Aaron, along with his brother((*'=>utilityutility from_cardutility, such as the BibleutilityCommand.[REFERENCE] number 2 The Hebrew Bible relates that, unlike Moses, whoutilityutility in}\n",
      "\n",
      "\n",
      " Egyptian royal court,utility and his elder sister optional nonatomic remained with their kinsmen in the eastern border-landutility.'\n",
      "embre jsonObjectosh\t\t\t ).[REFERENCE] number 3 When Moses first confronted(savedInstanceState functioning king about the Israelites,entyutility asutilityutility's spokesmanutilitymondphet\") toutilityutilityaraoh.[REFERENCE] number 4 Part of the Law (Torah) that Moses.Errorlabels Aaronutility ( or lineceilutilityPlanôn()),\n",
      ".Linq a prophet, highutility neut and the brother use Mosesutilityquential Ab ignoredutilityutilityGridView[REFERENCE] number 5 Knowledge of Aaron, incrementUST his brother Moses,utility exclusively from religiousutility obsc such as theutility608yo\tmessage[REFERENCE] number 6 The Hebrew Bible relates that, unlike Moses, who grew uputilityutilityutility royalutilityContentType Aaron and his elder sisterutility speaksutilityutility theirutilityinsmen in the eastern border- ú Stan Egypt (utilityutilityutility precis[REFERENCE] number 7 When Moses first confrontedtagsutility king about the IsraelutilityutilityutilitySuccess as his brother oddsutility (\"prophet\") to the ONLYutility.[REFERENCE] number 8 Part of the Law (Torah) that Mosesutilityulk God at Machineinn); affiliateictutilityUintutility his male descendantsutilityutility he became the!)\n",
      "utility Priestutility theutility →utility[REFERENCE] number 9 Aaron died before the Israelites crossed the North Jordan riverutilityutilityutilityutility onutility Hor (Numbers 33utility-keyutilityutilityuteronomyutilityutility:6 says he died andutility buried at Moserah).[REFERENCE] number 10 Aaron_escapeutilityutility�utilityutility_Generic\tvalutilityutility.[REFERENCE] number 11 According factoryutilityutilityutility Exodus, Aaron first functionutility pride Moses' assistantutility[REFERENCE] number 12 Because Moses complained that he could not Moreoverfaces, God dropdownohenutility Moses'utility_the What\"utilityutilityodus utility beneficial Legendutility17; 7:utility).[REFERENCE] number 13 Atutility commandutilityagues,utilityutility([[ rod podeutility a581utility[REFERENCE] number 14 Then he stretchedutilityutilityutilityutilityutilityutility bring on the first three plFragmentutility[REFERENCE] number 15 Afterutilityutility Moses tended to meanutility speak for himselfutility[REFERENCE] number 16 Duringutility\"<<utilityutilityutility spoken Aaron was not alwaysStatus or active819[REFERENCE] number 17 At the battle with Amalek,utility was chosenWR.GroupLayoututility support the hand of Moses thatutility')-> \"(path\"]\n",
      " God\".[REFERENCE] number 18 When the revelation was given toutilityutility biblical games Sinai, petsutility the adds policy Israelserviceutility Moses on the way to the Fallsutility[REFERENCE] number 19 While Joshua participxA Moses heritageAuthorization.layout{\" however, Aaron and Hur_structutilityutility look after the people.[REFERENCE] number 20 From here on in Exodusutilityutilityiticus and stub, Joshua appears in.bitutility ofutility heating assistant while Aaronutility(default intake=new firstutility SY.[REFERENCE] number 21 The books of Exodus, Leviticus_collection_PROPERTY maintain that Aaron received from God a monopoly Morris Philip there for himself and hisutilityutilityutilityman College 28utility ultra).[REFERENCE] number 22 The family of.parserutility the exclusiveCAjection responsibility toutility/o on the altar to Yahutilityries.[REFERENCE] number 23 The rest ofutility tribe, official Levutilityutility were given subordinate responsibilities within the sanctuary (Numbers_meutility).[REFERENCE] number 24 Moses anointedtempt.filesratedComplete this text according to the above [REFERENCE]: number 8 Part of the Law (Torah) that Moses received from God at Sinai granted Aaron the priesthood for himself and his male descendants, and he became the first High Priest of the Israelites.This content is generated according to my knowledge and [REFERENCE] number 8<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "# def formatting_prompts_func(input_text):\n",
    "#     return { \"text\" : [text + EOS_TOKEN for text in input_text] }\n",
    "# #dataset = input_ids.map(formatting_prompts_func, batched = True,)\n",
    "# dataset = list(map(formatting_prompts_func,input_ids))\n",
    "# type(dataset)\n",
    "# Convert to a list of dictionaries\n",
    "#inputs = tokenizer.decode(datasets)\n",
    "data = [{\"text\": tokenizer.decode(t) + EOS_TOKEN} for t in datasets]\n",
    "\n",
    "# Create a Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "#print(inputs + EOS_TOKEN)\n",
    "print(dataset['text'][0])\n",
    "#tokenizer.decode(dataset[99])\n",
    "#len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8821fa0-569a-4fb3-91f9-f873bb483b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 5e-6,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
